---
title: "ISYE6501 Homework 2"
author: ''
date: "1/20/2019"
output:
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Question 3.1
## (a) cv
Find a good classifier using ksvm or kknn, include cross-validation (a) and splitting the data sets (b).

First try the cross validation with kknn

```{r question3.1_a}

library(kknn)
rm(list = ls())
set.seed(42) # the answer of life
ccdata <- read.delim("~/Documents/R/GeorgiaTech/Validation/credit_card_data-headers.txt")

# preview data
head(ccdata)
# test multiple k-values
max_k <- 100
max_k_folds <- 20
# vector to save predictions
accuracy <- c()
# now calculate the accuracy for kmax neighbors
for (neighbor in 1:max_k) {
    # this package includes cross validation
    model <- cv.kknn(R1~.,ccdata, kcv=10, # use the common k-folds (Dr. Sokol suggested it)
            k=neighbor, 
            scale=TRUE) 
    # round it to make it comparable
    pred <- round(model[[1]][,2])
    # show as percentage
    accuracy <- c(accuracy, round(sum(pred == ccdata$R1)/nrow(ccdata), digits = 4)*100)
}
# show accuracy for each k
print (accuracy)
# see how accuracy behaves
plot(accuracy, ylab="prediction accuracy",xlab="K values")
lines(accuracy)
# this is used to print a table in 4 columns
half <- max_k/2
matrix <- cbind(seq(1,half), accuracy[1:half],seq((half+1), max_k), accuracy[(half+1):50])
colnames(matrix) <- c("k", "accuracy %", "k", "accuracy %")
library(knitr) # table library
table <- kable(matrix)
print (table)

```
Here we can look the accuracy for each K. The graph shows that accuracy with k < 5 are the worst and then it behaves similarly. There are multiple k values, that have a small accuracy difference. After k > 20 accuracy seems to decrease (also look plot).Cross validation helps you make better use of your data, and the averaging process helps give a better estimate of model quality. Cross validation is not used to pick a model but to make better use of the data and get a better estimate of model quality.

Our model determines that the best k is the following one
```{r, echo=FALSE}
print (which.max(accuracy))
```
With an accuracy of 
```{r, echo=FALSE}
paste(max(accuracy), "%")
```

The kknn tells us that the 12 nearest neighboors help to classify/cluster with `86.09%` accuracy. When I compare the kknn in Homework 1 this accuracy is minimal better and the k-value is the same. Also I tested the model with different k-folds, I'm not leaving the proof of this because it will be too long, but the result accuracy is the same but the k varies. In this case the best k is affected by the k-folds/groups selected but this is not relevant. According to wikipedia `10-fold cross-validation is commonly used, but in general k remains an unfixed parameter.` 

Finally I learned that the kknn in Homework 1 did also cross validation k = 654. When k = n (the number of observations), the k-fold cross-validation is exactly the leave-one-out cross-validation.

## (b) Now we split the data (kknn)

```{r question3.1_b}
# another student suggested this library to split the data, so let's try it
library(caTools)
library(kernlab)
rm(list = ls())
set.seed(42) # the answer of life
ccdata <- read.delim("~/Documents/R/GeorgiaTech/Validation/credit_card_data-headers.txt")
# split 75% training, 7.5% validation and 7.5% test
sample = sample.split(ccdata, SplitRatio = 0.75)
train = subset(ccdata, sample == TRUE)
remaining  = subset(ccdata, sample == FALSE)
# split in half
sample2 = sample.split(remaining, SplitRatio = .50)
validation = subset(remaining, sample2 == TRUE)
test = subset(remaining, sample2 == FALSE)

# verify

is_equals <- nrow(test) + nrow(validation) + nrow(train) == nrow(ccdata)
print (is_equals) # is valid
#pick first KNN
accuracy <- c()
for (k in 1:50) {

  # model knn using training and validation
  model_knn <- kknn(R1~.,train,validation,k=k,scale=TRUE)

	#  compare with validation
  prediction <-  round(model_knn$fitted.values)
  accuracy<-c(accuracy,sum(prediction == validation$R1) / nrow(validation))
}
print (paste("Best model is", which.max(accuracy)))
print (paste("Best accuracy is ", max(accuracy)*100, "%"))

```
```{r plot_b, echo=FALSE}
plot(accuracy, ylab="prediction accuracy",xlab="K values")
lines(accuracy)
```
```{r}
# now compare quality with test data 
model_knn <- kknn(R1~.,train,test, k=which.max(accuracy), scale=TRUE)
# round to make it comparable
pred <- round(model_knn$fitted.values)
# calculate performance
ac <- sum(pred == test$R1) / nrow(test)
print (paste("KNN performance on test data is ",ac*100, "%"))

```
## (b) now compare it with the SVM

The valid way to compare it is against the performance on the test
```{r}
# make vector with c values to test
C_values <- c(10^(-7:7))
acc_svm <- c()
for (i in 1:15) {

	# fit model using training set
  model_ksvm <- ksvm(train$R1~., data=train, type = "C-svc", kernel = "vanilladot",
  C = C_values[i],
  scaled=TRUE) 

	#  accuracy model using validation set
  pred <- predict(model_ksvm, validation[,1:10])
  acc_svm <- c(acc_svm, sum(pred == validation$R1) / nrow(validation))
}

print (paste("Best C ksvm ",C_values[which.max(acc_svm)]))
print (paste("KSVM quality on validation data  ",max(acc_svm)*100))
model_scaled <- ksvm(train$R1~., data=train,
	 type = "C-svc",
  kernel = "vanilladot", #could test with other non linear kernel
  C = C_values[which.max(acc_svm)], #pick the best model
	 scaled=TRUE)

# estimate real quality/performance
ac <- sum(predict(model_scaled,test[,1:10]) == test$R1) / nrow(test)
print (paste("KSVM Quality on test data is ",ac*100, "%"))


```

We can say that the ksvm has better performance on validation data, and we can't use the test performance to compare the models because if we use the same data to pick the best model as we do to estimate how good the best one is, the model will appear to be better than it really is.

So we actually pick the `ksvm` as the best (validation data) and the estimated performance of the model is `80.61%` (test data).

# Question 4.1

In my current work (telephone company) we use clustering algorithms to identify which cellphone numbers are related to the same family. In this case each family is a cluster we want to identify. For our company it is valuable to know this, so we can make specialized offers to the members of the family and also it is used to contact only one of them instead of randomly call all the members of the family (we don't want to spam our 9 million users). The predictors we currently use are the following.

1. The location at night (we suppose that people that sleeps in the same location can be family)
2. The most numbers they have calls in. (We have this as a relative continous variable in the database)
3. The most numbers they have calls out.(We have this as a relative continous variable in the database.)
4. The owners of the cellphone line (we suppose that children (<18) have phones and their parents are the owners, also as a relative continous variable)
5. The most SMS sent (as relative continous variable, we suppose that family text each other frequently)

# Question 4.2
Use the R function kmeans to cluster the points as well as possible. Report the best combination of
predictors, your suggested value of k, and how well your best clustering predicts flower type.

```{r question4.2}
rm(list = ls())
set.seed(42) # the answer of life
# default dataset in R, this has the response variable but it should be used only to compare
data <- iris
# according to the TA we can plot the data to have an idea how we can cluster
library(ggplot2)
normalize <- function(x){
  return ((x-min(x))/(max(x)-min(x)))
}
# we should scale the data, mean = 0 and stdev = 1
data.scaled<- data[,c(1,2,3,4)]
data.scaled$Sepal.Length<- normalize(data.scaled$Sepal.Length)
data.scaled$Sepal.Width<- normalize(data.scaled$Sepal.Width)
data.scaled$Petal.Length<- normalize(data.scaled$Petal.Length)
data.scaled$Petal.Width<- normalize(data.scaled$Petal.Width)
# after testing a few plots, this give us 3 good potential groups 
#ggplot(data , aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
#ggplot(data , aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
#ggplot(data , aes(Sepal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(data , aes(Petal.Length, Sepal.Width, color = Species)) + geom_point()

range<-1:10
n <-20 #Run the K Means algorithm n times
avg <-integer(length(range)) #Set up an empty vector to hold all of points
avg2 <- integer(length(range))
for(k in range){ # For each value of the range variable
 value <-integer(n) #Set up an empty vector to hold the n tries
 within_cluster_sum<-integer(n)
 for(i in 1:n){
   k.temp <-kmeans(data.scaled[c(1,4)],centers=k, nstart = 10) #Run kmeans
   # cluster sum of squares by cluster
   value[i] <-k.temp$betweenss/k.temp$totss*100 #store accuracy
   within_cluster_sum[i] <- k.temp$tot.withinss
 }
 avg[k] <-mean(value) #Average the n total accuracy
 avg2[k] <-mean(within_cluster_sum) #Average the n total accuracy
}
# see the values and the plot
avg
plot(range,avg,type="b", main="Accuracy of clustering",
 ylab="Accuracy",
 xlab="Value of K")
# accuracy here is: measure of the total distance between points and their cluster centers
# we can see that when k=4, accuracy is around 90%
plot(range,avg2,type="b", main="Elbow diagram",
 ylab="Average Total Within Sum of Squares",
 xlab="Value of K")
# compare other k's with Species
cluster3 <- kmeans(data.scaled[c(1,4)],3, nstart = 10)
cluster4 <- kmeans(data.scaled[c(1,4)],4, nstart = 10)
cluster5 <- kmeans(data.scaled[c(1,4)],5, nstart = 10)
# now use the actual clustering that we shouldn't know
# here we can compare how the kmeans clutered the species
table(cluster3$cluster, data$Species)
table(cluster4$cluster, data$Species)
table(cluster5$cluster, data$Species)

# plot cluster

ggplot(data, aes(Sepal.Length, Petal.Width, color = cluster4$cluster)) + geom_point()

```

## Analysis of Clustering

In this problem we had 4 predictors to find the best cluster. Those predictors were Sepal.Length, Sepal.Width, Petal.Length and Petal.Width. To have better results I scaled the data (stdev=1). 

To know which predictors to use, I followed the suggestion from the TA. Graph the predictors with `ggplot2` and see which one would generate some easy to divide group. So I tested and found out that using `Sepal.Length, Petal.Width` or using `Petal.Length, Petal.Width` could be good enough as predictors.


Then I tested multiple k's from 1 to 10. Then there were two types of measuring how good was the clusters (kmeans result). 

  1. Measure of the total distance between points and their cluster centers.
  2. In this data set, we had the actual cluster each data point belongs to, but we won't have this data on a real clustering problem. 
  
  So I focused more on the first measuring type.
  
  I found I way to measure the accuracy of the total distance between points by
  this equation = `The between-cluster sum of squares/The total sum of squares`. 
  
  In the first plot we can see that when k >= 3, the accuracy is `>94%`. Take notice that here there is no need to use the best accuracy we have, because we only need to know a good cluster enough for our needs. So I recommend to use k = 4 to cluster our iris data.
  
  To confirm this I made the `Elbow diagram` using `Total within-cluster sum of squares` and I can confirm that when k >= 3, the cluster is good enough. 
  
  Finally I used the table to use the classification of Species and confirm the clustering. With k = 4, setosa is all clustered in one group, versicolor in 2 groups and virginica almost in 2 groups.
  


