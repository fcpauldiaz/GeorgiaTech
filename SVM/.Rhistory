t_value <- beta.estimate/std.error  # calc t statistic
p_value <- 2*pt(-abs(t_value), df=nrow(data)-ncol(data))  # calc p Value
print (t_value)
print (p_value)
for (predictorName in attr(modelSummary$coefficients)) {
print (predictorName)
}
usercrime <- read.delim("~/Dropbox/UGT/IntroModeling/HW5/usercrime.txt", header=TRUE)
data = usercrime
# which variables should I use ? Are every variable correlated to crime
# how do I choose the variables ?
linearMod <- lm(Crime ~ ., data=data)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
beta.estimate <- modelCoeffs["M", "Estimate"]  # get beta estimate for M
std.error <- modelCoeffs["M", "Std. Error"]  # get std.error for M
t_value <- beta.estimate/std.error  # calc t statistic
p_value <- 2*pt(-abs(t_value), df=nrow(data)-ncol(data))  # calc p Value
print (t_value)
print (p_value)
print (modelSummary$coefficients[1])
usercrime <- read.delim("~/Dropbox/UGT/IntroModeling/HW5/usercrime.txt", header=TRUE)
data = usercrime
# which variables should I use ? Are every variable correlated to crime
# how do I choose the variables ?
linearMod <- lm(Crime ~ ., data=data)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
beta.estimate <- modelCoeffs["M", "Estimate"]  # get beta estimate for M
std.error <- modelCoeffs["M", "Std. Error"]  # get std.error for M
t_value <- beta.estimate/std.error  # calc t statistic
p_value <- 2*pt(-abs(t_value), df=nrow(data)-ncol(data))  # calc p Value
print (t_value)
print (p_value)
print (modelSummary$coefficients[[1]])
usercrime <- read.delim("~/Dropbox/UGT/IntroModeling/HW5/usercrime.txt", header=TRUE)
data = usercrime
# which variables should I use ? Are every variable correlated to crime
# how do I choose the variables ?
linearMod <- lm(Crime ~ ., data=data)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
beta.estimate <- modelCoeffs["M", "Estimate"]  # get beta estimate for M
std.error <- modelCoeffs["M", "Std. Error"]  # get std.error for M
t_value <- beta.estimate/std.error  # calc t statistic
p_value <- 2*pt(-abs(t_value), df=nrow(data)-ncol(data))  # calc p Value
print (t_value)
print (p_value)
print (modelSummary$coefficients[2])
usercrime <- read.delim("~/Dropbox/UGT/IntroModeling/HW5/usercrime.txt", header=TRUE)
data = usercrime
# which variables should I use ? Are every variable correlated to crime
# how do I choose the variables ?
linearMod <- lm(Crime ~ ., data=data)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
beta.estimate <- modelCoeffs["M", "Estimate"]  # get beta estimate for M
std.error <- modelCoeffs["M", "Std. Error"]  # get std.error for M
t_value <- beta.estimate/std.error  # calc t statistic
p_value <- 2*pt(-abs(t_value), df=nrow(data)-ncol(data))  # calc p Value
print (t_value)
print (p_value)
print (modelSummary$coefficients[1][2])
usercrime <- read.delim("~/Dropbox/UGT/IntroModeling/HW5/usercrime.txt", header=TRUE)
data = usercrime
# which variables should I use ? Are every variable correlated to crime
# how do I choose the variables ?
linearMod <- lm(Crime ~ ., data=data)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
beta.estimate <- modelCoeffs["M", "Estimate"]  # get beta estimate for M
std.error <- modelCoeffs["M", "Std. Error"]  # get std.error for M
t_value <- beta.estimate/std.error  # calc t statistic
p_value <- 2*pt(-abs(t_value), df=nrow(data)-ncol(data))  # calc p Value
print (t_value)
print (p_value)
print (modelSummary$coefficients[[2]])
usercrime <- read.delim("~/Dropbox/UGT/IntroModeling/HW5/usercrime.txt", header=TRUE)
data = usercrime
# which variables should I use ? Are every variable correlated to crime
# how do I choose the variables ?
linearMod <- lm(Crime ~  M + Ed + U2 + Ineq + Prob, data=data)
modelSummary <- summary(linearMod)
modelSummary
usercrime <- read.delim("~/Dropbox/UGT/IntroModeling/HW5/usercrime.txt", header=TRUE)
data = usercrime
# which variables should I use ? Are every variable correlated to crime
# how do I choose the variables ?
linearMod <- lm(Crime ~ ., data=data)
modelSummary <- summary(linearMod)
print (modelSummary)
usercrime <- read.delim("~/Dropbox/UGT/IntroModeling/HW5/usercrime.txt", header=TRUE)
data = usercrime
# which variables should I use ? Are every variable correlated to crime
# how do I choose the variables ?
linearMod <- lm(Crime ~  M + Ed + U2 + Ineq + Prob + Po1, data=data)
modelSummary <- summary(linearMod)  # capture model summary as an object
print (modelSummary)
install.packages("rmarkdown")
---
title: "Hmw1"
author: "fcpauldiaz"
date: "1/12/2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## R Markdown
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r cars}
summary(cars)
```
## Including Plots
You can also embed plots, for example:
```{r pressure, echo=FALSE}
plot(pressure)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
credit_card_data.headers <- read.delim("~/Documents/R/GeorgiaTec/SVM/credit_card_data-headers.txt", header=FALSE)
View(credit_card_data.headers)
credit_card_data.headers <- read.delim("~/Documents/R/GeorgiaTec/SVM/credit_card_data-headers.txt", header=FALSE)
credit_card_data.headers <- read.delim("~/Documents/R/GeorgiaTec/SVM/credit_card_data-headers.txt", header=FALSE)
View(credit_card_data.headers)
data <- read.delim("./credit_card_data-headers.txt")
script.dir <- dirname(sys.frame(1)$ofile)
this.dir <- dirname(parent.frame(2)$ofile)
setwd("~/Documents/R/GeorgiaTec/SVM")
Â©
data <- read.delim("./credit_card_data-headers.txt")
dat
View(data)
model <- ksvm(R1 ~ ., data=data, type="C-svc", kernel="vanilladot", C=100, scaled=TRUE)
library(kernlab)
model <- ksvm(R1 ~ ., data=data, type="C-svc", kernel="vanilladot", C=100, scaled=TRUE)
attributes(model)
a <- colSums(model@xmatrix[[1]] * model@coef[[1]])
a
a0 <- model@b
a0
pred <- predict(model,data[,1:10])
sum(pred == data[,11]) / nrow(data)
model <- ksvm(R1 ~ ., data=data, type="C-svc", kernel="vanilladot", C=10*10^8, scaled=TRUE)
pred <- predict(model,data[,1:10])
sum(pred == data[,11]) / nrow(data)
model <- ksvm(R1 ~ ., data=data, type="C-svc", kernel="vanilladot", C=10*10^-8, scaled=TRUE)
pred <- predict(model,data[,1:10])
sum(pred == data[,11]) / nrow(data)
model <- ksvm(R1 ~ ., data=data, type="C-svc", kernel="vanilladot", C=90, scaled=TRUE)
pred <- predict(model,data[,1:10])
sum(pred == data[,11]) / nrow(data)
model <- ksvm(R1 ~ ., data=data, type="C-svc", kernel="vanilladot", C=100, scaled=TRUE)
model <- ksvm(R1 ~ ., data=data, type="C-svc", kernel="vanilladot", C=1000, scaled=TRUE)
pred <- predict(model,data[,1:10])
sum(pred == data[,11]) / nrow(data)
model <- ksvm(R1 ~ ., data=data, type="C-svc", kernel="vanilladot", C=50, scaled=TRUE)
pred <- predict(model,data[,1:10])
sum(pred == data[,11]) / nrow(data)
summary(model)
model
model@b
model@xmatrix
model_scaled <- model
predicted_scaled<-rep(0,nrow(data))
#For each data point, perform the transformation, calculate a*scaled(data point)+a0,
#and predict value of data point based on the resulting value
for (i in 1:nrow(data)){
#If the data point is above the classifier, predicted value = 1
if (sum(a_scaled*(data[i,1:10]-model_scaled@scaling$x.scale$`scaled:center`)/model_scaled@scaling$x.scale$`scaled:scale`) + a0_scaled >= 0){
predicted_scaled[i] <- 1
}
#If the data point is below the classifier, predicted value = 0
if (sum(a_scaled*(data[i,1:10]-model_scaled@scaling$x.scale$`scaled:center`)/model_scaled@scaling$x.scale$`scaled:scale`) + a0_scaled < 0){
predicted_scaled[i] <- 0
}
}
a_scaled = a
a0_scaled = a0
predicted_scaled<-rep(0,nrow(data))
#For each data point, perform the transformation, calculate a*scaled(data point)+a0,
#and predict value of data point based on the resulting value
for (i in 1:nrow(data)){
#If the data point is above the classifier, predicted value = 1
if (sum(a_scaled*(data[i,1:10]-model_scaled@scaling$x.scale$`scaled:center`)/model_scaled@scaling$x.scale$`scaled:scale`) + a0_scaled >= 0){
predicted_scaled[i] <- 1
}
#If the data point is below the classifier, predicted value = 0
if (sum(a_scaled*(data[i,1:10]-model_scaled@scaling$x.scale$`scaled:center`)/model_scaled@scaling$x.scale$`scaled:scale`) + a0_scaled < 0){
predicted_scaled[i] <- 0
}
}
predicted_scaled
View(predicted_scaled)
# -------------------- Code for Question 2.2 part 3 -----------------------------
# Clear environment
rm(list = ls())
#First, load the kknn library (which contains the kknn function) and read in the data
#
library(kknn)
data <- read.delim("~/Dropbox/UGT /IntroModeling/HW1/credit_card_data-headers.txt")
#
# optional check to make sure the data is read correctly
#
head(data)
##   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11
## 1 1 30.83 0.000 1.25 1 0 1 1 202 0 1
## 2 0 58.67 4.460 3.04 1 0 6 1 43 560 1
## 3 0 24.50 0.500 1.50 1 1 0 1 280 824 1
## 4 1 27.83 1.540 3.75 1 0 5 0 100 3 1
## 5 1 20.17 5.625 1.71 1 1 0 1 120 0 1
## 6 1 32.08 4.000 2.50 1 1 0 0 360 0 1
# NOTE: ALL ROWS OF THIS FILE STARTING WITH "##" DENOTE R OUTPUT
#
# Create a function to calculate the accuracy of the model with k=X
#
check_accuracy = function(X){
predicted <- rep(0,(nrow(data))) # predictions: start with a vector of all zeros
# for each row, estimate its response based on the other rows
for (i in 1:nrow(data)){
# data[-i] means we remove row i of the data when finding nearest neighbors...
#...otherwise, it'll be its own nearest neighbor!
model=kknn(R11~.,data[-i,],data[i,],k=X, scale = TRUE) # use scaled data
# record whether the prediction is at least 0.5 (round to one) or less than 0.5 (round to zero)
predicted[i] <- as.integer(fitted(model)+0.5) # round off to 0 or 1
}
# calculate fraction of correct predictions
accuracy = sum(predicted == data[,11]) / nrow(data)
return(accuracy)
}
#
# Now call the function for values of k from 1 to 20 (you could try higher values of k too)
#
acc <- rep(0,20) # set up a vector of 20 zeros to start
for (X in 1:20){
acc[X] = check_accuracy(X) # test knn with X neighbors
}
#
# report accuracies
#
acc
##          [,1]      [,2]      [,3]      [,4]     [,5]      [,6]      [,7]      [,8]
##[1,] 0.8149847 0.8149847 0.8149847 0.8149847 0.851682 0.8455657 0.8470948 0.8486239
##          [,9]     [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]    [,17]
##[1,] 0.8470948 0.8501529 0.851682 0.853211 0.851682 0.851682 0.853211 0.851682 0.851682
##        [,18]     [,19]     [,20]
##[1,] 0.851682 0.8501529 0.8501529l
# -------------------- Code for Question 2.2 part 3 -----------------------------
# Clear environment
rm(list = ls())
#First, load the kknn library (which contains the kknn function) and read in the data
#
library(kknn)
data <- read.delim("~/Dropbox/UGT /IntroModeling/HW1/credit_card_data-headers.txt")
#
# optional check to make sure the data is read correctly
#
head(data)
##   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11
## 1 1 30.83 0.000 1.25 1 0 1 1 202 0 1
## 2 0 58.67 4.460 3.04 1 0 6 1 43 560 1
## 3 0 24.50 0.500 1.50 1 1 0 1 280 824 1
## 4 1 27.83 1.540 3.75 1 0 5 0 100 3 1
## 5 1 20.17 5.625 1.71 1 1 0 1 120 0 1
## 6 1 32.08 4.000 2.50 1 1 0 0 360 0 1
# NOTE: ALL ROWS OF THIS FILE STARTING WITH "##" DENOTE R OUTPUT
#
# Create a function to calculate the accuracy of the model with k=X
#
check_accuracy = function(X){
predicted <- rep(0,(nrow(data))) # predictions: start with a vector of all zeros
# for each row, estimate its response based on the other rows
for (i in 1:nrow(data)){
# data[-i] means we remove row i of the data when finding nearest neighbors...
#...otherwise, it'll be its own nearest neighbor!
model=kknn(R11~.,data[-i,],data[i,],k=X, scale = TRUE) # use scaled data
# record whether the prediction is at least 0.5 (round to one) or less than 0.5 (round to zero)
predicted[i] <- as.integer(fitted(model)+0.5) # round off to 0 or 1
}
# calculate fraction of correct predictions
accuracy = sum(predicted == data[,11]) / nrow(data)
return(accuracy)
}
#
# Now call the function for values of k from 1 to 20 (you could try higher values of k too)
#
acc <- rep(0,20) # set up a vector of 20 zeros to start
for (X in 1:20){
acc[X] = check_accuracy(X) # test knn with X neighbors
}
#
# report accuracies
#
acc
##          [,1]      [,2]      [,3]      [,4]     [,5]      [,6]      [,7]      [,8]
##[1,] 0.8149847 0.8149847 0.8149847 0.8149847 0.851682 0.8455657 0.8470948 0.8486239
##          [,9]     [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]    [,17]
##[1,] 0.8470948 0.8501529 0.851682 0.853211 0.851682 0.851682 0.853211 0.851682 0.851682
##        [,18]     [,19]     [,20]
##[1,] 0.851682 0.8501529 0.8501529l
# -------------------- Code for Question 2.2 part 3 -----------------------------
# Clear environment
rm(list = ls())
#First, load the kknn library (which contains the kknn function) and read in the data
#
library(kknn)
data <- read.delim(".credit_card_data-headers.txt")
#
# optional check to make sure the data is read correctly
#
head(data)
##   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11
## 1 1 30.83 0.000 1.25 1 0 1 1 202 0 1
## 2 0 58.67 4.460 3.04 1 0 6 1 43 560 1
## 3 0 24.50 0.500 1.50 1 1 0 1 280 824 1
## 4 1 27.83 1.540 3.75 1 0 5 0 100 3 1
## 5 1 20.17 5.625 1.71 1 1 0 1 120 0 1
## 6 1 32.08 4.000 2.50 1 1 0 0 360 0 1
# NOTE: ALL ROWS OF THIS FILE STARTING WITH "##" DENOTE R OUTPUT
#
# Create a function to calculate the accuracy of the model with k=X
#
check_accuracy = function(X){
predicted <- rep(0,(nrow(data))) # predictions: start with a vector of all zeros
# for each row, estimate its response based on the other rows
for (i in 1:nrow(data)){
# data[-i] means we remove row i of the data when finding nearest neighbors...
#...otherwise, it'll be its own nearest neighbor!
model=kknn(R11~.,data[-i,],data[i,],k=X, scale = TRUE) # use scaled data
# record whether the prediction is at least 0.5 (round to one) or less than 0.5 (round to zero)
predicted[i] <- as.integer(fitted(model)+0.5) # round off to 0 or 1
}
# calculate fraction of correct predictions
accuracy = sum(predicted == data[,11]) / nrow(data)
return(accuracy)
}
#
# Now call the function for values of k from 1 to 20 (you could try higher values of k too)
#
acc <- rep(0,20) # set up a vector of 20 zeros to start
for (X in 1:20){
acc[X] = check_accuracy(X) # test knn with X neighbors
}
#
# report accuracies
#
acc
##          [,1]      [,2]      [,3]      [,4]     [,5]      [,6]      [,7]      [,8]
##[1,] 0.8149847 0.8149847 0.8149847 0.8149847 0.851682 0.8455657 0.8470948 0.8486239
##          [,9]     [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]    [,17]
##[1,] 0.8470948 0.8501529 0.851682 0.853211 0.851682 0.851682 0.853211 0.851682 0.851682
##        [,18]     [,19]     [,20]
##[1,] 0.851682 0.8501529 0.8501529l
# -------------------- Code for Question 2.2 part 3 -----------------------------
# Clear environment
rm(list = ls())
#First, load the kknn library (which contains the kknn function) and read in the data
#
library(kknn)
data <- read.delim("./credit_card_data-headers.txt")
#
# optional check to make sure the data is read correctly
#
head(data)
##   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11
## 1 1 30.83 0.000 1.25 1 0 1 1 202 0 1
## 2 0 58.67 4.460 3.04 1 0 6 1 43 560 1
## 3 0 24.50 0.500 1.50 1 1 0 1 280 824 1
## 4 1 27.83 1.540 3.75 1 0 5 0 100 3 1
## 5 1 20.17 5.625 1.71 1 1 0 1 120 0 1
## 6 1 32.08 4.000 2.50 1 1 0 0 360 0 1
# NOTE: ALL ROWS OF THIS FILE STARTING WITH "##" DENOTE R OUTPUT
#
# Create a function to calculate the accuracy of the model with k=X
#
check_accuracy = function(X){
predicted <- rep(0,(nrow(data))) # predictions: start with a vector of all zeros
# for each row, estimate its response based on the other rows
for (i in 1:nrow(data)){
# data[-i] means we remove row i of the data when finding nearest neighbors...
#...otherwise, it'll be its own nearest neighbor!
model=kknn(R11~.,data[-i,],data[i,],k=X, scale = TRUE) # use scaled data
# record whether the prediction is at least 0.5 (round to one) or less than 0.5 (round to zero)
predicted[i] <- as.integer(fitted(model)+0.5) # round off to 0 or 1
}
# calculate fraction of correct predictions
accuracy = sum(predicted == data[,11]) / nrow(data)
return(accuracy)
}
#
# Now call the function for values of k from 1 to 20 (you could try higher values of k too)
#
acc <- rep(0,20) # set up a vector of 20 zeros to start
for (X in 1:20){
acc[X] = check_accuracy(X) # test knn with X neighbors
}
#
# report accuracies
#
acc
##          [,1]      [,2]      [,3]      [,4]     [,5]      [,6]      [,7]      [,8]
##[1,] 0.8149847 0.8149847 0.8149847 0.8149847 0.851682 0.8455657 0.8470948 0.8486239
##          [,9]     [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]    [,17]
##[1,] 0.8470948 0.8501529 0.851682 0.853211 0.851682 0.851682 0.853211 0.851682 0.851682
##        [,18]     [,19]     [,20]
##[1,] 0.851682 0.8501529 0.8501529l
---
title: "Homework 1"
author: "Pablo Diaz"
date: "1/12/2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Question 2.1
I currently work in analytics at a telephone provider company. Classification is very important because we can offer the best offer to the classified customers based on their phone usage. One interesting model we could make is to predict when a user can move from a non contract plan to a contract plan. Some relevant predictors that we have stored on our databases ARPU (average revenue per user), this is the current amount of money they are paying us, Data MB (the amount of internet data they use), Calls in (the relative amount of time they receive calls), calls out (the relative amount of time they make calls) and Data User (variable to know if a user uses more than 90% of their data plan)
## Question 2.2
```{r question2.1}
library(kernlab)
# import data, set current directory like this: setwd("~/Documents/R/GeorgiaTech/SVM")
data <- read.delim("./credit_card_data-headers.txt")
# make results reproducible
set.seed(5)
# simple linear kernel, C values too small or too big decrease accuracy
model <- ksvm(R1 ~ ., data=data, type="C-svc", kernel="optimal", C=50, scaled=TRUE)
summary(model)
# calculate a1 to am mutiplying the xmatrix by the coeficient of the linear combination
a <- colSums(model@xmatrix[[1]] * model@coef[[1]])
# calculate a0
a0 <- model@b
# see what the model predicts
pred <- predict(model,data[,1:10])
# see what fraction of the modelâs predictions match the classification
output <- sum(pred == data[,11]) / nrow(data)
print (output)
#0.8639144
```
### Result 2.2
We found out that the best accuracy was `0.8629144`with multipe values of C
### Question 2.4
```{r question2.4}
```
library(kknn)
View(data)
nrow(data)
ncolumn(data)
ncol(data)
i <- 167
model_kknn=kknn(R1~.,data[-i,],data[i,],k=10, scale = TRUE)
model_kknn
model_kknn$response
fitted(model_kknn)
predict(model_kknn, data[i,])
plot(model_kknn)
as.matrix(table(Actual = cl, Predicted = fit))
model_kknn$fitted.values
(fitted(model_kknn)+0.5)
?kknn
round(0.6907101)
- as.integer(fitted(model)+0.5)
- as.integer(fitted(model_kknn)+0.5)
as.integer(fitted(model_kknn)+0.5)
View(data)
data[-i,11]
i=1
data[-i,11]
rep(0,(nrow(data)))
View( rep(0,(nrow(data))))
predictions <- c()
for (i in 1:nrow(data)) {
# remove row i, to avoid finding own nearnest neighbor
# data[-i,] is all the data except for the ith data point.
model_kknn=kknn(R1~.,data[-i,],data[i,],k=10, scale = TRUE) # use scaled data
# prediction
fit <- model_kknn$fitted.values
# should make it comparable with R1 column
predictions <- c(predictions, round(fit)) #save it in array
}
# check accuracy
accuracy = sum(predictions == data[,ncol(data)]) / nrow(data)
accuracy
View(predictions)
findBestK(3)
findBestK <- function(paramK) {
predictions <- c()
for (i in 1:nrow(data)) {
# remove row i, to avoid finding own nearnest neighbor
# data[-i,] is all the data except for the ith data point.
# maybe it is a good idea to divide in train an
model_kknn=kknn(R1~.,data[-i,],data[i,],k=paramK, scale = TRUE) # use scaled data
# prediction
fit <- model_kknn$fitted.values
# should make it comparable with R1 column
predictions <- c(predictions, round(fit)) #save it in array
}
# check accuracy
accuracy = sum(predictions == data[,ncol(data)]) / nrow(data)
return (accuracy)
}
findBestK(3)
findBestK(13)
findBestK(12)
findBestK(100)
findBestK(200)
findBestK(10^5)
findBestK(1000)
findBestK(100)
findBestK(101)
