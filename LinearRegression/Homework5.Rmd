---
title: "Homework5"
author: "Pablo"
date: "08/02/2019"
output:
  pdf_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 8.1

I work in analytics in a telephone company and the linear regression is a easy model to use in a production environment and easy to understand, so we use it regularly. For this question, I have a nice example. In our telephone company, the prepaid users are the majority of users and they make the most of the revenue of the company. The prepaid users are the ones without a contract and they recharge their phone voice plans or data plans according to their needs. So in this case we want to predict monthly sales of prepaid recharges using data from previous years. As possible predictors we can use the following ones:

1. The monthly recharges of voice plans (minutes).
2. The monthly recharges of data plans (GB).
3. The monthly new users we have.
4. The montly users we loose (churn).
5. The monthly active users.

# Question 8.2

use regression (a useful R function is lm or glm) to predict the observed crime rate in a city.

```{r}
set.seed(42)
# import data
uscrime <- read.delim("~/Documents/R/GeorgiaTech/DataPreparation/uscrime.txt")
# see graphically
plot(uscrime$Crime)
# we already know there are 2 outliers from homework 3 but it is a good practice to check for it.
boxplot(uscrime$Crime,col=rgb(0,0,1,0.5), main="Box plot of Crime")
# This graph shows that most of the data behaves normal and shows 2 possible outliers.
qqline(uscrime$Crime,col=rgb(0,0,1,0.5))
# build the model with all avaialble predictors.
model <- lm( Crime ~ ., uscrime)

# lets see 
print (summary(model))
```

## Analysis 8.2

To apply a linear regression model to a data set is pretty straight forward.

First use all the variable we have to build the model and we analyze the model there are some predictors with high `p-values` that we can remove because they won't be significant coefficients for the model. The adjusted $R^2$ value `0.7` seems high enough but by removing coefficients the model can be simplified. The `p-values` represent the probability of a coefficient being zero, so only keep the coefficients with relative low `p-values`.

```{r}
# use only the predictors that show a low probability of being zero
better_model <- lm(Crime ~ M + Ed + U2 + Ineq + Prob, uscrime) 
summary(better_model)

```


With this result the $R^2$ metric has lowered to `0.2`, maybe if instead of using a `0.05` threshold for the p-values it is used a `0.1` threshold, the $R^2$ gets better.


```{r}
better_model_2 <- lm(Crime ~ M + Ed + U2 +Po1+ Ineq + Prob, uscrime) 
summary(better_model_2)
```

With this change the adjusted $R^2$ value has risen to `0.73`, so the threshold of the p-value can have significant relevance over the model quality. It is better to use the adjusted $R^2$ because the other one will always increase when the predictors increase, the adjusted one takes into account the number of predictors used, so it is more reliable.

The adjusted $R^2$ value is not the unique measure for a linear regression model, let's try Dr. Sokol lectures and apply `Akaike’s information criterion - AIC and the Bayesian information criterion - BIC`

```{r}
# model 1
AIC(model)
BIC(model)

# model 2
AIC(better_model)
BIC(better_model)

# model 3
AIC(better_model_2)
BIC(better_model_2)

# The lower AIC/BIC is the best one, so better_model_2 is the best
```


Now let's try to predict, with the given data and the models.

```{r}
data_point <-data.frame(M=14.0,So=0, Ed = 10.0, Po1 = 12.0, Po2=15.5,LF = 0.640, M.F=94.0, Pop = 150, NW=1.1, U1 = 0.120, U2=3.6, Wealth = 3200, Ineq = 20.1, Prob = 0.040,Time=39.0)

pred_model1 <- predict(model, data_point)
pred_model1

pred_model2 <- predict(better_model,data_point)
pred_model2

pred_model3 <- predict(better_model_2, data_point)
pred_model3

# basic stats to compare the predictions
avg <- mean(uscrime$Crime)
mx <- max(uscrime$Crime)
mn <- min(uscrime$Crime)
avg
mx
mn
```


As it can be seen, the predicted value with the first model `155` seems not real because it is less than the minimum value of the whole data set. The predicted value with model 2 and 3, are near the average and between the range, so those values make sense. The best model still is model 3 because has lower AIC, BIC and better adjusted $R^2$.

The equation of the best model is the following one:

$ y = -5040.50 + 105.02M + 196.47Ed+ 89.39U2 + 115.02Po1 + 67.65Ineq - 3801.84Prob $


In linear regression it is important to understand what the coefficients mean, so here it is the description on each coefficient used.

M = 	percentage of males aged 14–24 in total state population
Ed = 	mean years of schooling of the population aged 25 years or over
Po1 = per capita expenditure on police protection in 1960
U2 = unemployment rate of urban males 35–39
Ineq = 	income inequality: percentage of families earning below half the median income.
Prob = 	probability of imprisonment: ratio of number of commitments to number of offenses

Seeing this information it could be very tempting to say, for example, unemployment leads to crime, but remember that correlation doesn't mean causation.

### Regression output


```{r}
# confidence interval
confint(better_model_2)
# Adjusted R squared
summary(better_model_2)$adj.r.squared
# Residual Standard Error (RSE)
RSE <- sigma(better_model_2)
RSE
error_rate <- RSE/avg
```

The quality of a linear regression fit is typically assessed using two quantities: the residual standard error (RSE) and the $R^2$.

The $R^2$ can be interpreted as `73%` of the variance in the measure of crime can be predicted by M, Ed, Po1, U2, Ineq and Prob.

The RSE/mean can be interpreted as `23%` errror rate of the model (The less is better)

The `1%` threshold chosen for the p-values means that each coefficient will have `1%` of chance of not being significant.
 
### Cross Validation

Since the data set only has 47 points, it is highly probably that the reported quality of the model is overfitted so let's do cross validation to have a more realistic quality.

```{r}
# TA suggestion
library(DAAG)
# m is the number of folds. The m value was chosen arbitrarily
cross_lm <- cv.lm(uscrime,better_model_2,m=10)

# measure quality by calculating R^2 and RSE
#R^2 = 1 - RSE/TSS

```

$R^2 = (TSS-RSS)/TSS = 1 - RSS/TSS$
$RSE = \sqrt{\frac{1}{n-2} RSS}$

Where TSS is the total sum of squares.  $\sum_{i=1}^n ({y_i}-\bar{y})^2$
and RSS is the residual sum of squares. $\sum_{i=1}^n ({y_i}-\hat{y})^2$

```{r}
RSS <- attr(cross_lm,"ms")*nrow(uscrime)
TSS <- sum((uscrime$Crime - mean(uscrime$Crime))^2)
R_square <- 1 - RSS/TSS
R_square
RSE <- sqrt((1/(nrow(uscrime)-2))*RSS)
RSE
error_rate <- RSE/avg
error_rate
```

### Regression output CV

Now with cross-validation we have a more realistic fit of the model selected

The $R^2$ can be interpreted as `65%` of the variance in the measure of crime can be predicted by M, Ed, Po1, U2, Ineq and Prob. Here we have the not overfitted quality, and according to Dr. Sokol, it is still good enough to use it.

The RSE/mean can be interpreted as `26%` errror rate of the model (The less is better). This measure only changed `3%`.

Also tested cv with multiple `m` folds and the results varied 1%, so it was not sensible to m.
